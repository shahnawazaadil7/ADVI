{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is a machine learning technique that teaches software to make decisions by rewarding desired actions and punishing undesired ones. It's based on the idea that learning happens through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts of Reinforcement Learning\n",
    "\n",
    "Agent: The learner or decision-maker.\n",
    "\n",
    "Environment: Everything the agent interacts with.\n",
    "\n",
    "State: A specific situation in which the agent finds itself.\n",
    "\n",
    "Action: All possible moves the agent can make.\n",
    "\n",
    "Reward: Feedback from the environment based on the action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.\n",
    "\n",
    "For example, we have two classes Class 0 and Class 1 if the value of the logistic function for an input is greater than 0.5 (threshold value) then it belongs to Class 1 otherwise it belongs to Class 0. Itâ€™s referred to as regression because it is the extension of linear regression but is mainly used for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'fox': [-8.7274835e-03  2.1301603e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281435e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986944e-03 -6.8730689e-03 -4.9994136e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033188e-03 -2.7436304e-03 -8.3628418e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441387e-03 -1.7069983e-03\n",
      " -8.9569995e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668323e-03  3.1185509e-03 -9.5707225e-03\n",
      "  1.4764380e-03  6.5244650e-03  5.7464195e-03 -8.7630628e-03\n",
      " -4.5171450e-03 -8.1401607e-03  4.5955181e-05  9.2636319e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610616e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703888e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161046e-03 -5.9258699e-04  7.4888230e-03\n",
      " -6.9751980e-04 -1.6249418e-03  2.7443981e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361032e-03 -9.5840879e-03  2.4462652e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669201e-03 -7.7365185e-03\n",
      "  8.3959224e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430834e-03  2.6350426e-03  7.4271200e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583753e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856869e-03  5.7358933e-03 -7.7733753e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061312e-03  2.6675223e-04  3.8572431e-03\n",
      "  7.3857834e-03 -6.7251683e-03  5.5844807e-03 -9.5222257e-03\n",
      " -8.0446003e-04 -8.6887386e-03 -5.0986744e-03  9.2892265e-03\n",
      " -1.8582630e-03  2.9144264e-03  9.0712784e-03  8.9381309e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044296e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758990e-03]\n",
      "Similar words to 'fox': [('brown', 0.16694684326648712), ('lazy', 0.13887983560562134), ('dog', 0.13147231936454773)]\n",
      "\n",
      "Vocabulary: {'the': 0, 'is': 1, 'cat': 2, 'dog': 3, 'lazy': 4, 'fox': 5, 'quick': 6, 'at': 7, 'barks': 8, 'over': 9, 'jumps': 10, 'brown': 11}\n",
      "Number of words in vocabulary: 12\n",
      "\n",
      "Shape of all vectors: (12, 100)\n",
      "\n",
      "First 10 words in vocab: ['the', 'is', 'cat', 'dog', 'lazy', 'fox', 'quick', 'at', 'barks', 'over']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "                  \n",
    "# Sample sentences (replace with your own data)\n",
    "sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"the\", \"dog\", \"barks\", \"at\", \"the\", \"cat\"],\n",
    "    [\"the\", \"fox\", \"is\", \"quick\"],\n",
    "    [\"the\", \"cat\", \"is\", \"lazy\"]\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # Adjust parameters as needed\n",
    "# Parameters explained:\n",
    "# - sentences: A list of sentences (list of lists of words).  This is your training data.\n",
    "# - vector_size: Dimensionality of the word vectors (e.g., 100, 200, 300). Higher values capture more semantic information but require more data.\n",
    "# - window: Maximum distance between the current and predicted word within a sentence.\n",
    "# - min_count: Ignores all words with total frequency lower than this.  Helpful for removing rare words.\n",
    "# - sg: Training algorithm: 0 for CBOW (Continuous Bag-of-Words), 1 for skip-gram. CBOW is generally faster, skip-gram is better for infrequent words.\n",
    "\n",
    "# Get the vector for a word\n",
    "word = \"fox\"\n",
    "if word in model.wv:  # Check if the word is in the vocabulary\n",
    "    vector = model.wv[word]\n",
    "    print(f\"Vector for '{word}': {vector}\")\n",
    "else:\n",
    "    print(f\"The word '{word}' is not in the vocabulary.\")\n",
    "\n",
    "\n",
    "# Get similar words\n",
    "if word in model.wv:\n",
    "    similar_words = model.wv.most_similar(word, topn=3)  # Get top 3 similar words\n",
    "    print(f\"Similar words to '{word}': {similar_words}\")\n",
    "else:\n",
    "    print(f\"The word '{word}' is not in the vocabulary.\")\n",
    "\n",
    "# Save the model (optional)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load a saved model (optional)\n",
    "# loaded_model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Example of accessing the vocabulary:\n",
    "vocabulary = model.wv.key_to_index  # or model.wv.vocab  (older versions)\n",
    "print(f\"\\nVocabulary: {vocabulary}\")\n",
    "print(f\"Number of words in vocabulary: {len(vocabulary)}\")\n",
    "\n",
    "# Example of getting all vectors:\n",
    "all_vectors = model.wv.vectors\n",
    "print(f\"\\nShape of all vectors: {all_vectors.shape}\") # (vocabulary size, vector_size)\n",
    "\n",
    "# Example of getting all words:\n",
    "all_words = model.wv.index_to_key\n",
    "print(f\"\\nFirst 10 words in vocab: {all_words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
